# -*- coding: utf-8 -*-
"""linearRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S_z3jLOrfBATrmxGmNPXkyhnnEnbO9jH
"""

import pandas as pd
file = pd.read_csv("/content/Lifestyle_and_Health_Risk_Prediction_Synthetic_Dataset.csv")
file.head()

"""# Data cleaning
we need to change:

medium high lows ---> 1 2 0

yes no           ---> 1 0

profession --> one hot encoding     

sleep, age, bmi --> normalize

weight, height ---> drop it we already have bmi

and remove nan values

make train test split

  - random shuffle
  - split
"""

file.info() #we got lucky no null here
file.dropna(inplace=True)

file["exercise"].unique()

file.replace({'yes': 0, 'no': 1}, inplace=True)
file.replace({'low': 0, 'medium': 0.5, 'high': 1}, inplace=True)
file.replace({'high': 1, 'low': 0,'none':-1}, inplace=True)

file.drop("weight",inplace=True,axis=1)
file.drop("height",inplace=True,axis =1)

file.head()

def normalize(column,df):
  x_max, x_min = max(df[column]), min(df[column])
  df[column] = (df[column] - x_min) / (x_max - x_min)

normalize("sleep",file)
normalize("bmi",file)
normalize("age",file)

file.head()

def train_test_split(seed,fraction,df):
  df = df.sample(frac=1, random_state=seed).reset_index(drop=True)
  split = int(fraction * len(df))
  train_df = df[:split]
  test_df = df[split:].reset_index(drop=True)
  return [train_df,test_df]

train_df, test_df = train_test_split(69,0.85,file)

X_train, y_train = train_df.drop("health_risk",axis=1),train_df["health_risk"]
X_test, y_test = test_df.drop("health_risk",axis=1),test_df["health_risk"]
X_test

file["profession"].unique()

def one_hot_encode(column,df):
  categories = df[column].unique()
  for category in categories:
    df[category] = (df[column] == category).astype(int)
  df.drop(column,axis=1,inplace=True)

one_hot_encode("profession",X_train)
one_hot_encode("profession",X_test)

X_train

import numpy as np
weights = np.random.randn(16,1)
weights

X_train = X_train.to_numpy()
X_train

# Got too lazy
X_test = X_test.to_numpy()
y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

def train(epoch,learning_rate,X,y,weights):
  m = y.shape[0]
  for i in range(epoch):
    y_hat = np.dot(X,weights)
    dJ = (2/m)*(X.T).dot(y_hat-y)
    weights = weights - learning_rate*dJ
    if i%100 == 0:
      RMSE = np.sqrt(np.mean((y_hat-y)**2))
      print(f"Epoch {i}: RMSE : {RMSE}")
  return weights

train(2000,0.001,X_train,y_train,weights)

def test(X,y_true,weights):
  y_predicted = np.dot(X,weights)
  return y_predicted

y_predicted = test(X_test,y_test,weights)
def r2_score(y_hat, y):
    ss_res = np.sum((y - y_hat)**2)
    ss_tot = np.sum((y - np.mean(y))**2)
    return 1 - (ss_res / ss_tot)
def rmse(y_hat, y):
    return np.sqrt(np.mean((y_hat - y)**2))

print(f"RMSE on test: {rmse(y_predicted,y_test)}\nR2 on test: {r2_score(y_predicted,y_test)}")